{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mds8Yeyon3IO",
        "outputId": "251daad4-b6f0-40ac-d236-06eeabd46ce4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 841 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Total 91 (delta 0), reused 0 (delta 0), pack-reused 91\u001b[K\n",
            "Unpacking objects: 100% (91/91), done.\n",
            "Installing konlpy.....\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2021-12-12 08:54:44--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c0:3470, 2406:da00:ff00::3403:4be7, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=Y61OwNAIdNN%2BfdzsfEXTLOBxiGM%3D&Expires=1639300646&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-12-12 08:54:45--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=Y61OwNAIdNN%2BfdzsfEXTLOBxiGM%3D&Expires=1639300646&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.128.17\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.128.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  3.58MB/s    in 0.4s    \n",
            "\n",
            "2021-12-12 08:54:45 (3.58 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2021-12-12 08:55:57--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::3403:4be7, 2406:da00:ff00::22cd:e0db, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=DZGXlZOwKtlOMUUpGqI%2Bx67JW%2F0%3D&Expires=1639300735&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-12-12 08:55:57--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=DZGXlZOwKtlOMUUpGqI%2Bx67JW%2F0%3D&Expires=1639300735&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.88.91\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.88.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  31.6MB/s    in 1.5s    \n",
            "\n",
            "2021-12-12 08:55:59 (31.6 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
            "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim konlpy\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "!bash Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab190912.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rkxek2YhLot"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.cuda as cuda\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from google.colab import drive\n",
        "from konlpy.tag import Mecab\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMP5I1KLmEPS",
        "outputId": "3ac0e2c0-fc4c-409f-8e4d-460de160b11b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training MNIST Model on cuda\n",
            "============================================\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'Training MNIST Model on {device}\\n{\"=\"*44}')\n",
        "\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Hu1rPyD6TsPs",
        "outputId": "1d39fed1-723d-4a6b-e91e-f5ea8aeb5fc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>content</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1427</td>\n",
              "      <td>꽤 볼만한 스릴러. 반전에 약간 실망할 수도.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>439</td>\n",
              "      <td>손으로 셀수없을만큼 여러번 봤어도 늘 같은 장면에서 눈물이 난다. 김양이 엄마 찾을...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1537</td>\n",
              "      <td>할복할 자신도 없으면서 푼돈이나 뜯으려는 찌질이...모토메의 사연을 몰랐을 땐 우리...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1334</td>\n",
              "      <td>애초에 천재설정 영화에서 감독의허황이라고 비판하는 사람들이 더 이해안감;;</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>53</td>\n",
              "      <td>젊을때의 그들, 어설프지만 풋풋한 모습.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3097</th>\n",
              "      <td>503</td>\n",
              "      <td>레드퍼드는 위대한 개츠비에도 그렇고 마지막에 뒤에서 총 맞는 장면이 여기서 있네</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3098</th>\n",
              "      <td>846</td>\n",
              "      <td>딱한마디만하겠습니다....... 최고!!!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3099</th>\n",
              "      <td>1269</td>\n",
              "      <td>영화관에 가서 보면 더 좋았을걸...어린시절로 돌아간 느낌이들어 만감이...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3100</th>\n",
              "      <td>512</td>\n",
              "      <td>세번째 아이 낳기 전에 아버지랑 마지막으로 만나 산책하는 장면에서 눈물...주인공이...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3101</th>\n",
              "      <td>17</td>\n",
              "      <td>와타나베 켄 연기에 보는 나까지 눈물남 ㅠㅠ 밥먹다 우는장면 보고 울컥했다 진짜</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3102 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0                                            content  label\n",
              "0           1427                          꽤 볼만한 스릴러. 반전에 약간 실망할 수도.      1\n",
              "1            439  손으로 셀수없을만큼 여러번 봤어도 늘 같은 장면에서 눈물이 난다. 김양이 엄마 찾을...      1\n",
              "2           1537  할복할 자신도 없으면서 푼돈이나 뜯으려는 찌질이...모토메의 사연을 몰랐을 땐 우리...      1\n",
              "3           1334          애초에 천재설정 영화에서 감독의허황이라고 비판하는 사람들이 더 이해안감;;      0\n",
              "4             53                             젊을때의 그들, 어설프지만 풋풋한 모습.      0\n",
              "...          ...                                                ...    ...\n",
              "3097         503       레드퍼드는 위대한 개츠비에도 그렇고 마지막에 뒤에서 총 맞는 장면이 여기서 있네      1\n",
              "3098         846                            딱한마디만하겠습니다....... 최고!!!      0\n",
              "3099        1269         영화관에 가서 보면 더 좋았을걸...어린시절로 돌아간 느낌이들어 만감이...      0\n",
              "3100         512  세번째 아이 낳기 전에 아버지랑 마지막으로 만나 산책하는 장면에서 눈물...주인공이...      1\n",
              "3101          17       와타나베 켄 연기에 보는 나까지 눈물남 ㅠㅠ 밥먹다 우는장면 보고 울컥했다 진짜      1\n",
              "\n",
              "[3102 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/drive/Shareddrives/실전기계_project/data/data_spo_aug_sr_train_1210.csv\")\n",
        "#df = pd.read_csv(\"/content/drive/Shareddrives/실전기계_project/data/data_spo_aug_rs_train_1210.csv\")\n",
        "#df = pd.read_csv(\"/content/drive/Shareddrives/실전기계_project/data/data_spo_aug_ri_train_1210.csv\")\n",
        "#df = pd.read_csv(\"/content/drive/Shareddrives/실전기계_project/data/data_spo_aug_rd_train_1210.csv\")\n",
        "#df = pd.read_csv(\"/content/drive/Shareddrives/실전기계_project/data/data_spo_aug_all_train_1210.csv\")\n",
        "#df = pd.read_csv(\"/content/drive/Shareddrives/실전기계_project/data/data_spo_aug_X_train_1210.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbqgWAJJQOa1"
      },
      "source": [
        "## For Down Sampling\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ue1uaB-vd6ks",
        "outputId": "d8424d62-9930-4be7-b466-7a87979365bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>content</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1168</th>\n",
              "      <td>913</td>\n",
              "      <td>이젠 시간여행을 하지않는다.내가 이날을 위 상 여행을 한 것처럼살아간다 이렇게 말했...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>875</th>\n",
              "      <td>925</td>\n",
              "      <td>인어공주의 사랑이야기가 신선하고 흥미로웠다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570</th>\n",
              "      <td>103</td>\n",
              "      <td>마지막 무지개다리를 건너,이든과 만낫을때 폭풍오열..우리아이와도 그렇게 만나게되겟죠</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1743</th>\n",
              "      <td>13</td>\n",
              "      <td>화려하게 정성들인 제사상이 왤케 슬프죠??ㅠㅠ 눈물샘 폭발 꺼이꺼이임지호님 어머니곁...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1095</th>\n",
              "      <td>753</td>\n",
              "      <td>오페라 좋아하는 사람을 이해할수가 없었는데 이 영화의 나를 울게하소서 이 하나만으로...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3079</th>\n",
              "      <td>610</td>\n",
              "      <td>시즌1에서는 미국 스타일의 키 큰 여자가 최종회에서 죽었다면 최상의 전</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2173</th>\n",
              "      <td>1049</td>\n",
              "      <td>정말 멋진 결말!ㅎ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2233</th>\n",
              "      <td>378</td>\n",
              "      <td>느낌이 오래도록 가는 영화... 사와지리 에리카는 정말 세 하나는 잘 보는듯.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2584</th>\n",
              "      <td>109</td>\n",
              "      <td>벌써 마지막회라니 믿기싫은현실ㅜ 넘 재밌어요</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1699</th>\n",
              "      <td>33</td>\n",
              "      <td>답답하고, 냉정한 도시. 영화가 만들어진 97년에 비해 지금은 사람답게 살 수 있는...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3102 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0                                            content  label\n",
              "1168         913  이젠 시간여행을 하지않는다.내가 이날을 위 상 여행을 한 것처럼살아간다 이렇게 말했...      1\n",
              "875          925                           인어공주의 사랑이야기가 신선하고 흥미로웠다.      0\n",
              "570          103     마지막 무지개다리를 건너,이든과 만낫을때 폭풍오열..우리아이와도 그렇게 만나게되겟죠      1\n",
              "1743          13  화려하게 정성들인 제사상이 왤케 슬프죠??ㅠㅠ 눈물샘 폭발 꺼이꺼이임지호님 어머니곁...      1\n",
              "1095         753  오페라 좋아하는 사람을 이해할수가 없었는데 이 영화의 나를 울게하소서 이 하나만으로...      0\n",
              "...          ...                                                ...    ...\n",
              "3079         610            시즌1에서는 미국 스타일의 키 큰 여자가 최종회에서 죽었다면 최상의 전      1\n",
              "2173        1049                                         정말 멋진 결말!ㅎ      0\n",
              "2233         378        느낌이 오래도록 가는 영화... 사와지리 에리카는 정말 세 하나는 잘 보는듯.      0\n",
              "2584         109                           벌써 마지막회라니 믿기싫은현실ㅜ 넘 재밌어요      0\n",
              "1699          33  답답하고, 냉정한 도시. 영화가 만들어진 97년에 비해 지금은 사람답게 살 수 있는...      0\n",
              "\n",
              "[3102 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# If needed down sampling\n",
        "np.random.seed(42)\n",
        "\n",
        "df_0 = df[df['label'] == 0]\n",
        "df_1 = df[df['label'] == 1]\n",
        "\n",
        "min_len = min(len(df_0), len(df_1))\n",
        "\n",
        "df = pd.concat([df_0[:min_len], df_1[:min_len]])\n",
        "df = df.sample(frac=1, random_state=42)\n",
        "\n",
        "train_df = df\n",
        "test_df = pd.read_csv(\"/content/drive/Shareddrives/실전기계_project/data/data_spo_test_1211.csv\")\n",
        "\n",
        "TRAIN_SET_LENGTH = len(train_df)\n",
        "TEST_SET_LENGTH = len(test_df)\n",
        "\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYETn1HanKud"
      },
      "outputs": [],
      "source": [
        "document = []\n",
        "mecab = Mecab()\n",
        "\n",
        "# document tokenize\n",
        "for doc in train_df['content']:\n",
        "  document.append(mecab.morphs(doc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um6RbRySd4gt"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "  def __init__(self, data, target, length, transform=None):\n",
        "    super(TextDataset, self).__init__()\n",
        "\n",
        "    self.data = data\n",
        "    self.target = target\n",
        "    self.length = length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.target)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx], self.target[idx], self.length[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocxlXN3SSUH2"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyDM13myEh-i"
      },
      "outputs": [],
      "source": [
        "#BATCH_SIZES = [32]  # no arg\n",
        "BATCH_SIZES = [64]  # arg\n",
        "INPUT_SIZES = [32, 64]\n",
        "HIDDEN_SIZES = [64, 128]\n",
        "LEARNING_RATES = [1e-3, 3e-3, 5e-3]\n",
        "WEIGHTDECAY = 0.9\n",
        "EPOCH = 40\n",
        "TRY = 3\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFyitQMdPa98"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoF2nc8MHnpq"
      },
      "outputs": [],
      "source": [
        "def train_test(BATCH_SIZE, INPUT_SIZE, HIDDEN_SIZE, LR, TRY):\n",
        "  global train_df, test_df, document, WEIGHTDECAY\n",
        "\n",
        "  tr_a, tr_l, te_a, te_l = None, None, None, None\n",
        "\n",
        "  # word2vec train\n",
        "  model = Word2Vec(document, size=INPUT_SIZE, window=5, min_count=1, sg=1, workers=2)\n",
        "\n",
        "  def make_tensor(df):\n",
        "    word_arr = [mecab.morphs(text) for text in df['content']]\n",
        "    data = [torch.Tensor([model.wv[word] for word in words if word in model.wv]) for words in word_arr]\n",
        "    \n",
        "    # for debug when test data words not in word2vec dictionary\n",
        "    #for i, d in enumerate(data):\n",
        "    #  if len(d) == 0:\n",
        "    #    print(df.loc[i, 'content'])\n",
        "    \n",
        "    doc_length = torch.LongTensor(list(map(lambda x: len(x), data)))\n",
        "    target = torch.FloatTensor(df['label'].to_numpy())\n",
        "    data = pad_sequence(data, batch_first=True)\n",
        "      \n",
        "    return data, doc_length, target\n",
        "\n",
        "  train_data, train_doc_length, train_target = make_tensor(train_df)\n",
        "  test_data, test_doc_length, test_target = make_tensor(test_df)\n",
        "\n",
        "  train_dataloader = DataLoader(TextDataset(train_data, train_target, train_doc_length), batch_size=BATCH_SIZE, shuffle=True)\n",
        "  test_dataloader = DataLoader(TextDataset(test_data, test_target, test_doc_length), batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "  class RNN(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(RNN, self).__init__()\n",
        "      self.lstm = nn.RNN(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers=2, bidirectional=True, dropout=0.3, batch_first=True)\n",
        "      self.linear_1 = nn.Linear(HIDDEN_SIZE * 2, 64)\n",
        "      self.linear_2 = nn.Linear(64, 1)\n",
        "      self.batchnorm_1 = nn.BatchNorm1d(64)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, length):\n",
        "      packed_input = pack_padded_sequence(x, length.tolist(), batch_first=True, enforce_sorted=False)\n",
        "      packed_outputs, hidden = self.lstm(packed_input)\n",
        "      output, output_length = pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "      last_seq_idxs = torch.LongTensor([x - 1 for x in output_length])\n",
        "      output = output[range(output.shape[0]), last_seq_idxs, :]\n",
        "      x = self.batchnorm_1(self.relu(self.linear_1(output)))\n",
        "      x = self.sigmoid(self.linear_2(x))\n",
        "      return x.view(-1)\n",
        "\n",
        "  import os\n",
        "  os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "  model = RNN()\n",
        "  model.to(device)\n",
        "\n",
        "  def weight_init_kaiming_normal(submodule):\n",
        "    if isinstance(submodule, torch.nn.RNN) or isinstance(submodule, torch.nn.LSTM):\n",
        "      for name, param in submodule.named_parameters():\n",
        "        if 'weight_ih' in name:\n",
        "            torch.nn.init.xavier_uniform_(param.data)\n",
        "        elif 'weight_hh' in name:\n",
        "            torch.nn.init.orthogonal_(param.data)\n",
        "        elif 'bias' in name:\n",
        "            param.data.fill_(0)\n",
        "    elif isinstance(submodule, torch.nn.Linear):\n",
        "      torch.nn.init.kaiming_normal_(submodule.weight)\n",
        "      submodule.bias.data.fill_(0.01)\n",
        "    elif isinstance(submodule, torch.nn.BatchNorm1d):\n",
        "      submodule.weight.data.fill_(1.0)\n",
        "      submodule.bias.data.zero_()\n",
        "  \n",
        "  model.apply(weight_init_kaiming_normal)\n",
        "\n",
        "  criterion = F.binary_cross_entropy\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHTDECAY)\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [8, 16, 24, 32], gamma=0.5)\n",
        "\n",
        "  # train\n",
        "  for epoch in range(EPOCH):\n",
        "    corrent_results_sum = 0.0\n",
        "    avarage_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "      inputs, labels, length = data\n",
        "      inputs, labels, length = inputs.to(device), labels.to(device), length\n",
        "      y_pred = model(inputs, length)\n",
        "      loss = criterion(y_pred, labels, reduction='mean')\n",
        "\n",
        "      y_pred_tag = torch.round(y_pred)\n",
        "      corrent_results_sum += (y_pred_tag == labels).sum().float()\n",
        "      avarage_loss += loss.item()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      scheduler.step()\n",
        "    \n",
        "    #if epoch % 10 == 0:\n",
        "      #print(f\"Epoch: {epoch + 1} | Acc: {corrent_results_sum * 100 / TRAIN_SET_LENGTH} | Loss: {avarage_loss / len(train_dataloader)}\")\n",
        "    \n",
        "    if epoch == EPOCH - 1:\n",
        "      tr_a, tr_l = corrent_results_sum * 100 / TRAIN_SET_LENGTH, avarage_loss / len(train_dataloader)\n",
        "      print(f\"Try: {TRY} | Train Acc: {corrent_results_sum * 100 / TRAIN_SET_LENGTH} | Train Loss: {avarage_loss / len(train_dataloader)}\")\n",
        "\n",
        "\n",
        "  # test\n",
        "  corrent_results_sum = 0.0\n",
        "  avarage_loss = 0.0\n",
        "  for i, data in enumerate(test_dataloader):\n",
        "    inputs, labels, length = data\n",
        "    inputs, labels, length = inputs.to(device), labels.to(device), length\n",
        "    y_pred = model(inputs, length)\n",
        "    loss = criterion(y_pred, labels, reduction='mean')\n",
        "\n",
        "    y_pred_tag = torch.round(y_pred)\n",
        "    corrent_results_sum += (y_pred_tag == labels).sum().float()\n",
        "    avarage_loss += loss.item()\n",
        "\n",
        "  te_a, te_l = corrent_results_sum * 100 / TEST_SET_LENGTH, avarage_loss / len(test_dataloader)\n",
        "  print(f\"Try: {TRY} | Test Acc: {corrent_results_sum * 100 / TEST_SET_LENGTH} | Test Loss: {avarage_loss / len(test_dataloader)}\")\n",
        "\n",
        "  return tr_a, tr_l, te_a, te_l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx6JRmpvATNF",
        "outputId": "e801b1f2-820b-4804-e790-9d6aef44566c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case => Batch Size: 64 | Input Size: 32 | Hidden Size: 64 | Learning Rate: 0.001\n",
            "Try: 1 | Train Acc: 85.3642807006836 | Train Loss: 0.3536658068092502\n",
            "Try: 1 | Test Acc: 70.45454406738281 | Test Loss: 0.6407657265663147\n",
            "Try: 2 | Train Acc: 84.46163177490234 | Train Loss: 0.35896322861009716\n",
            "Try: 2 | Test Acc: 68.63636016845703 | Test Loss: 0.6189238876104355\n",
            "Try: 3 | Train Acc: 83.5267562866211 | Train Loss: 0.3711549274775447\n",
            "Try: 3 | Test Acc: 69.54545593261719 | Test Loss: 0.6593561619520187\n",
            "Total Average => Train Acc: 84.45088958740234 | Train Loss: 0.36126132096563074 | Test Acc: 69.54545593261719 | Test Loss: 0.6396819253762563\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 32 | Hidden Size: 64 | Learning Rate: 0.003\n",
            "Try: 1 | Train Acc: 88.2333984375 | Train Loss: 0.27520874811678514\n",
            "Try: 1 | Test Acc: 70.90908813476562 | Test Loss: 0.7317611873149872\n",
            "Try: 2 | Train Acc: 89.45841217041016 | Train Loss: 0.24951863106416197\n",
            "Try: 2 | Test Acc: 68.18181610107422 | Test Loss: 0.7131298780441284\n",
            "Try: 3 | Train Acc: 87.8787841796875 | Train Loss: 0.2853485884106889\n",
            "Try: 3 | Test Acc: 66.36363220214844 | Test Loss: 0.7129550278186798\n",
            "Total Average => Train Acc: 88.52352905273438 | Train Loss: 0.27002532253054534 | Test Acc: 68.48484802246094 | Test Loss: 0.7192820310592651\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 32 | Hidden Size: 64 | Learning Rate: 0.005\n",
            "Try: 1 | Train Acc: 90.65119171142578 | Train Loss: 0.23532554203150224\n",
            "Try: 1 | Test Acc: 64.54545593261719 | Test Loss: 0.888242095708847\n",
            "Try: 2 | Train Acc: 86.07350158691406 | Train Loss: 0.31027537523483745\n",
            "Try: 2 | Test Acc: 68.63636016845703 | Test Loss: 0.7429465055465698\n",
            "Try: 3 | Train Acc: 90.7479019165039 | Train Loss: 0.22930690190013575\n",
            "Try: 3 | Test Acc: 64.54545593261719 | Test Loss: 0.8557597249746323\n",
            "Total Average => Train Acc: 89.15753173828125 | Train Loss: 0.25830260638882513 | Test Acc: 65.90909576416016 | Test Loss: 0.8289827754100164\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 32 | Hidden Size: 128 | Learning Rate: 0.001\n",
            "Try: 1 | Train Acc: 93.391357421875 | Train Loss: 0.187383947627885\n",
            "Try: 1 | Test Acc: 65.45454406738281 | Test Loss: 0.815579742193222\n",
            "Try: 2 | Train Acc: 91.65054321289062 | Train Loss: 0.2208540474577826\n",
            "Try: 2 | Test Acc: 68.18181610107422 | Test Loss: 0.7824563235044479\n",
            "Try: 3 | Train Acc: 92.64990234375 | Train Loss: 0.20920368846581908\n",
            "Try: 3 | Test Acc: 64.54545593261719 | Test Loss: 0.8857297599315643\n",
            "Total Average => Train Acc: 92.56393432617188 | Train Loss: 0.20581389451716223 | Test Acc: 66.06060791015625 | Test Loss: 0.8279219418764114\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 32 | Hidden Size: 128 | Learning Rate: 0.003\n",
            "Try: 1 | Train Acc: 97.64667510986328 | Train Loss: 0.08940253125465646\n",
            "Try: 1 | Test Acc: 71.36363220214844 | Test Loss: 1.052753135561943\n",
            "Try: 2 | Train Acc: 96.64732360839844 | Train Loss: 0.11836878088664035\n",
            "Try: 2 | Test Acc: 65.45454406738281 | Test Loss: 1.1148304045200348\n",
            "Try: 3 | Train Acc: 95.61573028564453 | Train Loss: 0.1247489085154874\n",
            "Try: 3 | Test Acc: 64.54545593261719 | Test Loss: 1.1872445344924927\n",
            "Total Average => Train Acc: 96.63658142089844 | Train Loss: 0.1108400735522614 | Test Acc: 67.1212158203125 | Test Loss: 1.118276024858157\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 32 | Hidden Size: 128 | Learning Rate: 0.005\n",
            "Try: 1 | Train Acc: 96.06704711914062 | Train Loss: 0.11163908942621581\n",
            "Try: 1 | Test Acc: 67.7272720336914 | Test Loss: 1.177209585905075\n",
            "Try: 2 | Train Acc: 93.13346099853516 | Train Loss: 0.1763197785737563\n",
            "Try: 2 | Test Acc: 70.0 | Test Loss: 0.9006560295820236\n",
            "Try: 3 | Train Acc: 95.42230224609375 | Train Loss: 0.12520228645631246\n",
            "Try: 3 | Test Acc: 66.81817626953125 | Test Loss: 1.1777503192424774\n",
            "Total Average => Train Acc: 94.874267578125 | Train Loss: 0.1377203848187615 | Test Acc: 68.18181610107422 | Test Loss: 1.0852053115765254\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 64 | Hidden Size: 64 | Learning Rate: 0.001\n",
            "Try: 1 | Train Acc: 85.84783935546875 | Train Loss: 0.3357993133214055\n",
            "Try: 1 | Test Acc: 67.2727279663086 | Test Loss: 0.6555748656392097\n",
            "Try: 2 | Train Acc: 85.49322509765625 | Train Loss: 0.34210382888511737\n",
            "Try: 2 | Test Acc: 63.63636016845703 | Test Loss: 0.6972890794277191\n",
            "Try: 3 | Train Acc: 85.17085266113281 | Train Loss: 0.344473269825079\n",
            "Try: 3 | Test Acc: 69.54545593261719 | Test Loss: 0.7303595691919327\n",
            "Total Average => Train Acc: 85.50396728515625 | Train Loss: 0.3407921373438673 | Test Acc: 66.81818389892578 | Test Loss: 0.6944078380862871\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 64 | Hidden Size: 64 | Learning Rate: 0.003\n",
            "Try: 1 | Train Acc: 88.1689224243164 | Train Loss: 0.2880910814416652\n",
            "Try: 1 | Test Acc: 68.18181610107422 | Test Loss: 0.7427785247564316\n",
            "Try: 2 | Train Acc: 90.7479019165039 | Train Loss: 0.23420834723783998\n",
            "Try: 2 | Test Acc: 66.36363220214844 | Test Loss: 0.8311324566602707\n",
            "Try: 3 | Train Acc: 90.42552947998047 | Train Loss: 0.25371120824497573\n",
            "Try: 3 | Test Acc: 68.63636016845703 | Test Loss: 0.7292634397745132\n",
            "Total Average => Train Acc: 89.7807846069336 | Train Loss: 0.2586702123081603 | Test Acc: 67.72726440429688 | Test Loss: 0.7677248070637385\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 64 | Hidden Size: 64 | Learning Rate: 0.005\n",
            "Try: 1 | Train Acc: 88.74919128417969 | Train Loss: 0.26730141134894625\n",
            "Try: 1 | Test Acc: 68.63636016845703 | Test Loss: 0.8540911674499512\n",
            "Try: 2 | Train Acc: 88.10444641113281 | Train Loss: 0.27981302476659115\n",
            "Try: 2 | Test Acc: 68.63636016845703 | Test Loss: 0.7154144495725632\n",
            "Try: 3 | Train Acc: 91.5538330078125 | Train Loss: 0.2174081580371273\n",
            "Try: 3 | Test Acc: 63.18181610107422 | Test Loss: 0.9567113667726517\n",
            "Total Average => Train Acc: 89.46916198730469 | Train Loss: 0.2548408647175549 | Test Acc: 66.81817626953125 | Test Loss: 0.842072327931722\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 64 | Hidden Size: 128 | Learning Rate: 0.001\n",
            "Try: 1 | Train Acc: 92.32752990722656 | Train Loss: 0.19579882676504096\n",
            "Try: 1 | Test Acc: 66.81817626953125 | Test Loss: 0.8324433714151382\n",
            "Try: 2 | Train Acc: 93.97162628173828 | Train Loss: 0.17219853659673612\n",
            "Try: 2 | Test Acc: 64.09090423583984 | Test Loss: 0.8762727677822113\n",
            "Try: 3 | Train Acc: 92.16634368896484 | Train Loss: 0.20633145862696123\n",
            "Try: 3 | Test Acc: 65.0 | Test Loss: 0.8099671006202698\n",
            "Total Average => Train Acc: 92.82183837890625 | Train Loss: 0.19144294066291279 | Test Acc: 65.30303192138672 | Test Loss: 0.8395610799392065\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 64 | Hidden Size: 128 | Learning Rate: 0.003\n",
            "Try: 1 | Train Acc: 94.51966094970703 | Train Loss: 0.14934915760341955\n",
            "Try: 1 | Test Acc: 60.909088134765625 | Test Loss: 1.2355891168117523\n",
            "Try: 2 | Train Acc: 96.38941955566406 | Train Loss: 0.12127957660324719\n",
            "Try: 2 | Test Acc: 65.90908813476562 | Test Loss: 1.017462134361267\n",
            "Try: 3 | Train Acc: 79.65827941894531 | Train Loss: 0.4220176641734279\n",
            "Try: 3 | Test Acc: 69.09090423583984 | Test Loss: 0.6602545529603958\n",
            "Total Average => Train Acc: 90.18913269042969 | Train Loss: 0.23088213279336486 | Test Acc: 65.30303192138672 | Test Loss: 0.9711019347111384\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 64 | Hidden Size: 128 | Learning Rate: 0.005\n",
            "Try: 1 | Train Acc: 92.97227478027344 | Train Loss: 0.17725225887736495\n",
            "Try: 1 | Test Acc: 62.727272033691406 | Test Loss: 1.2983169108629227\n",
            "Try: 2 | Train Acc: 81.81817626953125 | Train Loss: 0.389391297588543\n",
            "Try: 2 | Test Acc: 71.36363220214844 | Test Loss: 0.6903436481952667\n",
            "Try: 3 | Train Acc: 92.64990234375 | Train Loss: 0.1894593875931234\n",
            "Try: 3 | Test Acc: 65.90908813476562 | Test Loss: 1.0092985183000565\n",
            "Total Average => Train Acc: 89.14678955078125 | Train Loss: 0.2520343146863438 | Test Acc: 66.66667175292969 | Test Loss: 0.9993196924527487\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for batch_size in BATCH_SIZES:\n",
        "  for input_size in INPUT_SIZES:\n",
        "    for hidden_size in HIDDEN_SIZES:\n",
        "      for lr in LEARNING_RATES:\n",
        "        train_acc, train_loss, test_acc, test_loss = 0.0, 0.0, 0.0, 0.0\n",
        "        print(f\"Case => Batch Size: {batch_size} | Input Size: {input_size} | Hidden Size: {hidden_size} | Learning Rate: {lr}\")\n",
        "        for t in range(1, TRY + 1):\n",
        "          tr_a, tr_l, te_a, te_l = train_test(batch_size, input_size, hidden_size, lr, t)\n",
        "          train_acc += tr_a\n",
        "          train_loss += tr_l\n",
        "          test_acc += te_a\n",
        "          test_loss += te_l\n",
        "        train_acc /= TRY\n",
        "        train_loss /= TRY\n",
        "        test_acc /= TRY\n",
        "        test_loss /= TRY\n",
        "        print(f\"Total Average => Train Acc: {train_acc} | Train Loss: {train_loss} | Test Acc: {test_acc} | Test Loss: {test_loss}\")\n",
        "        print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRBhMw0h2ZkZ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "8"
      ],
      "metadata": {
        "id": "ONGtE-kXjH-X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "rnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}