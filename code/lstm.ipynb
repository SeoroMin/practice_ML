{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mds8Yeyon3IO",
        "outputId": "60fbcee1-d662-4d6d-ae3d-f8c237c30b53"
      },
      "source": [
        "!pip install gensim konlpy\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "!bash Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 70.6 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Total 91 (delta 0), reused 0 (delta 0), pack-reused 91\u001b[K\n",
            "Unpacking objects: 100% (91/91), done.\n",
            "Installing konlpy.....\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2021-12-13 02:50:44--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22e9:9f55, 2406:da00:ff00::6b17:d1f5, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=PX%2B4DKlcPfCRGPC2YqJ9fNzcFtA%3D&Expires=1639365644&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-12-13 02:50:45--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=PX%2B4DKlcPfCRGPC2YqJ9fNzcFtA%3D&Expires=1639365644&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.102.91\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.102.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  1.20MB/s    in 1.1s    \n",
            "\n",
            "2021-12-13 02:50:46 (1.20 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2021-12-13 02:51:58--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22e9:9f55, 2406:da00:ff00::6b17:d1f5, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=9Rupw3PzEcrjfQFOj7BeGgOOdiQ%3D&Expires=1639365070&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-12-13 02:51:59--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=9Rupw3PzEcrjfQFOj7BeGgOOdiQ%3D&Expires=1639365070&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 54.231.132.185\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|54.231.132.185|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  12.6MB/s    in 4.7s    \n",
            "\n",
            "2021-12-13 02:52:05 (10.2 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
            "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rkxek2YhLot"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.cuda as cuda\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from google.colab import drive\n",
        "from konlpy.tag import Mecab\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMP5I1KLmEPS",
        "outputId": "e4e52d78-f3c8-4fd4-d52e-2949ae01e7b8"
      },
      "source": [
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'Training MNIST Model on {device}\\n{\"=\"*44}')\n",
        "\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MNIST Model on cuda\n",
            "============================================\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv(\"/content/drive/Shareddrives/실전기계_project/data/data_spo_aug_sr_train_1210.csv\")\n",
        "#df = pd.read_csv(\"/content/drive/Shareddrives/실전기계_project/data/data_spo_aug_rs_train_1210.csv\")\n",
        "#df = pd.read_csv(\"/content/drive/Shareddrives/실전기계_project/data/data_spo_aug_ri_train_1210.csv\")\n",
        "df = pd.read_csv(\"/content/drive/Shareddrives/실전기계_project/data/data_spo_aug_rd_train_1210.csv\")\n",
        "#df = pd.read_csv(\"/content/drive/Shareddrives/실전기계_project/data/data_spo_aug_all_train_1210.csv\")\n",
        "#df = pd.read_csv(\"/content/drive/Shareddrives/실전기계_project/data/data_spo_aug_X_train_1210.csv\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "Hu1rPyD6TsPs",
        "outputId": "ebeb1e2e-0c95-470c-dcb3-49d75d650fbb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>content</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1418</td>\n",
              "      <td>황야의 마녀 마지막까지 발암이라 보면서 많이 답답했지만 그거 빼면 최고</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1103</td>\n",
              "      <td>결말빼고는 아드레날린팍팍!! 남자들 강추임</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>624</td>\n",
              "      <td>영화 스토리도 탄탄하고..덴젤도 멋있고..짱짱!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>318</td>\n",
              "      <td>난 앨리가 스타 포기하고 돌아와서 노래부르면서 다니는 행복한 결말만 생각하고있었는데...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>467</td>\n",
              "      <td>40년전 영화인데 지금 봐도 개 쩌네 특히나 후반부 액션씬 쩐다 야외에서 소룡이 성...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3097</th>\n",
              "      <td>27</td>\n",
              "      <td>알파치노에 의한 영화.음악에 맞춰 아름다운 여인과 화상적으로 추는 모습은 인상적이다...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3098</th>\n",
              "      <td>1414</td>\n",
              "      <td>아녜스의 마지막 공연 '춘희'의 감동이 아직도 사라지지 않아요 발레의 아름다움을 한...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3099</th>\n",
              "      <td>382</td>\n",
              "      <td>영화사에서 최고의 엔딩 씬이 아닐까.. 웃기면서 마지막에 울리는</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3100</th>\n",
              "      <td>342</td>\n",
              "      <td>지루하지만 꽤 재밌었다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3101</th>\n",
              "      <td>286</td>\n",
              "      <td>캐스팅이 이렇게 완벽할순 없다.. 해피엔딩을 위한 과정들이..</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3102 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0                                            content  label\n",
              "0           1418            황야의 마녀 마지막까지 발암이라 보면서 많이 답답했지만 그거 빼면 최고      1\n",
              "1           1103                            결말빼고는 아드레날린팍팍!! 남자들 강추임      1\n",
              "2            624                         영화 스토리도 탄탄하고..덴젤도 멋있고..짱짱!      0\n",
              "3            318  난 앨리가 스타 포기하고 돌아와서 노래부르면서 다니는 행복한 결말만 생각하고있었는데...      1\n",
              "4            467  40년전 영화인데 지금 봐도 개 쩌네 특히나 후반부 액션씬 쩐다 야외에서 소룡이 성...      1\n",
              "...          ...                                                ...    ...\n",
              "3097          27  알파치노에 의한 영화.음악에 맞춰 아름다운 여인과 화상적으로 추는 모습은 인상적이다...      1\n",
              "3098        1414  아녜스의 마지막 공연 '춘희'의 감동이 아직도 사라지지 않아요 발레의 아름다움을 한...      1\n",
              "3099         382                영화사에서 최고의 엔딩 씬이 아닐까.. 웃기면서 마지막에 울리는      1\n",
              "3100         342                                       지루하지만 꽤 재밌었다      0\n",
              "3101         286                 캐스팅이 이렇게 완벽할순 없다.. 해피엔딩을 위한 과정들이..      1\n",
              "\n",
              "[3102 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For Down Sampling\n",
        "\n"
      ],
      "metadata": {
        "id": "CbqgWAJJQOa1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue1uaB-vd6ks",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "outputId": "5e4ee107-a48b-4db2-842e-da7df11a4ae8"
      },
      "source": [
        "# If needed down sampling\n",
        "np.random.seed(42)\n",
        "\n",
        "df_0 = df[df['label'] == 0]\n",
        "df_1 = df[df['label'] == 1]\n",
        "\n",
        "min_len = min(len(df_0), len(df_1))\n",
        "\n",
        "df = pd.concat([df_0[:min_len], df_1[:min_len]])\n",
        "df = df.sample(frac=1, random_state=42)\n",
        "\n",
        "train_df = df\n",
        "test_df = pd.read_csv(\"/content/drive/Shareddrives/실전기계_project/data/data_spo_test_1211.csv\")\n",
        "\n",
        "TRAIN_SET_LENGTH = len(train_df)\n",
        "TEST_SET_LENGTH = len(test_df)\n",
        "\n",
        "train_df"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>content</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1139</th>\n",
              "      <td>509</td>\n",
              "      <td>극장판 엔딩, 감독판 엔딩. 어느하나를 꼽는게 아닌,두 엔딩이 함께 존재함으로서 이...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>199</td>\n",
              "      <td>재밌나요? 더빙을 보는데 ㅠ자막이더좋긴한데</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>536</th>\n",
              "      <td>1461</td>\n",
              "      <td>따뜻한 영화네요. 실제 제 상황이랑 비슷해서 눈물이..이 영화의 행복한 결말이 정말...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1772</th>\n",
              "      <td>58</td>\n",
              "      <td>중반 이후로 흥미진진했다. 마지막까지 될지 몰라서 '제발 성공해라.. 제발!!' 이...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1111</th>\n",
              "      <td>982</td>\n",
              "      <td>통속적이지만 미국적 느와르의 정석과 연출이</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3083</th>\n",
              "      <td>554</td>\n",
              "      <td>매회 살인자가 나오지만 가족의 따뜻함을 물씬 느끼게 해주는 수사드라마</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2128</th>\n",
              "      <td>871</td>\n",
              "      <td>소년병의 삶에 대한 이해, 반군에 대한 새로운 시각 또는 편견무엇이 진실인지는 모르...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2204</th>\n",
              "      <td>104</td>\n",
              "      <td>재밋네요 달팽이가 빨라서 더 재밌었어요</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2546</th>\n",
              "      <td>609</td>\n",
              "      <td>피아니스트와 같은 동급 영화라생각합니다 보시면 후회없을거예요 실화영화이니요</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1669</th>\n",
              "      <td>678</td>\n",
              "      <td>별로 점수를 매길 수 없는 몇 안 되는 영화.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3102 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0                                            content  label\n",
              "1139         509  극장판 엔딩, 감독판 엔딩. 어느하나를 꼽는게 아닌,두 엔딩이 함께 존재함으로서 이...      1\n",
              "895          199                            재밌나요? 더빙을 보는데 ㅠ자막이더좋긴한데      0\n",
              "536         1461  따뜻한 영화네요. 실제 제 상황이랑 비슷해서 눈물이..이 영화의 행복한 결말이 정말...      1\n",
              "1772          58  중반 이후로 흥미진진했다. 마지막까지 될지 몰라서 '제발 성공해라.. 제발!!' 이...      1\n",
              "1111         982                            통속적이지만 미국적 느와르의 정석과 연출이      0\n",
              "...          ...                                                ...    ...\n",
              "3083         554             매회 살인자가 나오지만 가족의 따뜻함을 물씬 느끼게 해주는 수사드라마      1\n",
              "2128         871  소년병의 삶에 대한 이해, 반군에 대한 새로운 시각 또는 편견무엇이 진실인지는 모르...      0\n",
              "2204         104                              재밋네요 달팽이가 빨라서 더 재밌었어요      0\n",
              "2546         609          피아니스트와 같은 동급 영화라생각합니다 보시면 후회없을거예요 실화영화이니요      0\n",
              "1669         678                          별로 점수를 매길 수 없는 몇 안 되는 영화.      0\n",
              "\n",
              "[3102 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYETn1HanKud"
      },
      "source": [
        "document = []\n",
        "mecab = Mecab()\n",
        "\n",
        "# document tokenize\n",
        "for doc in train_df['content']:\n",
        "  document.append(mecab.morphs(doc))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um6RbRySd4gt"
      },
      "source": [
        "class TextDataset(Dataset):\n",
        "  def __init__(self, data, target, length, transform=None):\n",
        "    super(TextDataset, self).__init__()\n",
        "\n",
        "    self.data = data\n",
        "    self.target = target\n",
        "    self.length = length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.target)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx], self.target[idx], self.length[idx]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "ocxlXN3SSUH2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyDM13myEh-i"
      },
      "source": [
        "#BATCH_SIZES = [32]  # no arg\n",
        "BATCH_SIZES = [64]  # arg\n",
        "INPUT_SIZES = [32, 64]\n",
        "HIDDEN_SIZES = [64, 128]\n",
        "LEARNING_RATES = [1e-3, 3e-3, 5e-3]\n",
        "WEIGHTDECAY = 0.9\n",
        "EPOCH = 40\n",
        "TRY = 3\n",
        "SEED = 42"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "lFyitQMdPa98"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoF2nc8MHnpq"
      },
      "source": [
        "def train_test(BATCH_SIZE, INPUT_SIZE, HIDDEN_SIZE, LR, TRY):\n",
        "  global train_df, test_df, document, WEIGHTDECAY\n",
        "\n",
        "  tr_a, tr_l, te_a, te_l = None, None, None, None\n",
        "\n",
        "  # word2vec train\n",
        "  model = Word2Vec(document, size=INPUT_SIZE, window=5, min_count=1, sg=1, workers=2)\n",
        "\n",
        "  def make_tensor(df):\n",
        "    word_arr = [mecab.morphs(text) for text in df['content']]\n",
        "    data = [torch.Tensor([model.wv[word] for word in words if word in model.wv]) for words in word_arr]\n",
        "    \n",
        "    # for debug when test data words not in word2vec dictionary\n",
        "    #for i, d in enumerate(data):\n",
        "    #  if len(d) == 0:\n",
        "    #    print(df.loc[i, 'content'])\n",
        "    \n",
        "    doc_length = torch.LongTensor(list(map(lambda x: len(x), data)))\n",
        "    target = torch.FloatTensor(df['label'].to_numpy())\n",
        "    data = pad_sequence(data, batch_first=True)\n",
        "      \n",
        "    return data, doc_length, target\n",
        "\n",
        "  train_data, train_doc_length, train_target = make_tensor(train_df)\n",
        "  test_data, test_doc_length, test_target = make_tensor(test_df)\n",
        "\n",
        "  train_dataloader = DataLoader(TextDataset(train_data, train_target, train_doc_length), batch_size=BATCH_SIZE, shuffle=True)\n",
        "  test_dataloader = DataLoader(TextDataset(test_data, test_target, test_doc_length), batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "  class LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(LSTM, self).__init__()\n",
        "      self.lstm = nn.LSTM(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers=2, bidirectional=True, dropout=0.3, batch_first=True)\n",
        "      self.linear_1 = nn.Linear(HIDDEN_SIZE * 2, 64)\n",
        "      self.linear_2 = nn.Linear(64, 1)\n",
        "      self.batchnorm_1 = nn.BatchNorm1d(64)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, length):\n",
        "      packed_input = pack_padded_sequence(x, length.tolist(), batch_first=True, enforce_sorted=False)\n",
        "      packed_outputs, hidden = self.lstm(packed_input)\n",
        "      output, output_length = pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "      last_seq_idxs = torch.LongTensor([x - 1 for x in output_length])\n",
        "      output = output[range(output.shape[0]), last_seq_idxs, :]\n",
        "      x = self.batchnorm_1(self.relu(self.linear_1(output)))\n",
        "      x = self.sigmoid(self.linear_2(x))\n",
        "      return x.view(-1)\n",
        "\n",
        "  import os\n",
        "  os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "  model = LSTM()\n",
        "  model.to(device)\n",
        "\n",
        "  def weight_init_kaiming_normal(submodule):\n",
        "    if isinstance(submodule, torch.nn.RNN) or isinstance(submodule, torch.nn.LSTM):\n",
        "      for name, param in submodule.named_parameters():\n",
        "        if 'weight_ih' in name:\n",
        "            torch.nn.init.xavier_uniform_(param.data)\n",
        "        elif 'weight_hh' in name:\n",
        "            torch.nn.init.orthogonal_(param.data)\n",
        "        elif 'bias' in name:\n",
        "            param.data.fill_(0)\n",
        "    elif isinstance(submodule, torch.nn.Linear):\n",
        "      torch.nn.init.kaiming_normal_(submodule.weight)\n",
        "      submodule.bias.data.fill_(0.01)\n",
        "    elif isinstance(submodule, torch.nn.BatchNorm1d):\n",
        "      submodule.weight.data.fill_(1.0)\n",
        "      submodule.bias.data.zero_()\n",
        "  \n",
        "  model.apply(weight_init_kaiming_normal)\n",
        "\n",
        "  criterion = F.binary_cross_entropy\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHTDECAY)\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [8, 16, 24, 32], gamma=0.5)\n",
        "\n",
        "  # train\n",
        "  for epoch in range(EPOCH):\n",
        "    corrent_results_sum = 0.0\n",
        "    avarage_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "      inputs, labels, length = data\n",
        "      inputs, labels, length = inputs.to(device), labels.to(device), length\n",
        "      y_pred = model(inputs, length)\n",
        "      loss = criterion(y_pred, labels, reduction='mean')\n",
        "\n",
        "      y_pred_tag = torch.round(y_pred)\n",
        "      corrent_results_sum += (y_pred_tag == labels).sum().float()\n",
        "      avarage_loss += loss.item()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      scheduler.step()\n",
        "    \n",
        "    #if epoch % 10 == 0:\n",
        "      #print(f\"Epoch: {epoch + 1} | Acc: {corrent_results_sum * 100 / TRAIN_SET_LENGTH} | Loss: {avarage_loss / len(train_dataloader)}\")\n",
        "    \n",
        "    if epoch == EPOCH - 1:\n",
        "      tr_a, tr_l = corrent_results_sum * 100 / TRAIN_SET_LENGTH, avarage_loss / len(train_dataloader)\n",
        "      print(f\"Try: {TRY} | Train Acc: {corrent_results_sum * 100 / TRAIN_SET_LENGTH} | Train Loss: {avarage_loss / len(train_dataloader)}\")\n",
        "\n",
        "\n",
        "  # test\n",
        "  corrent_results_sum = 0.0\n",
        "  avarage_loss = 0.0\n",
        "  for i, data in enumerate(test_dataloader):\n",
        "    inputs, labels, length = data\n",
        "    inputs, labels, length = inputs.to(device), labels.to(device), length\n",
        "    y_pred = model(inputs, length)\n",
        "    loss = criterion(y_pred, labels, reduction='mean')\n",
        "\n",
        "    y_pred_tag = torch.round(y_pred)\n",
        "    corrent_results_sum += (y_pred_tag == labels).sum().float()\n",
        "    avarage_loss += loss.item()\n",
        "\n",
        "  te_a, te_l = corrent_results_sum * 100 / TEST_SET_LENGTH, avarage_loss / len(test_dataloader)\n",
        "  print(f\"Try: {TRY} | Test Acc: {corrent_results_sum * 100 / TEST_SET_LENGTH} | Test Loss: {avarage_loss / len(test_dataloader)}\")\n",
        "\n",
        "  return tr_a, tr_l, te_a, te_l"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx6JRmpvATNF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24365b99-2178-4ab0-b88c-0c58359f010e"
      },
      "source": [
        "for batch_size in BATCH_SIZES:\n",
        "  for input_size in INPUT_SIZES:\n",
        "    for hidden_size in HIDDEN_SIZES:\n",
        "      for lr in LEARNING_RATES:\n",
        "        train_acc, train_loss, test_acc, test_loss = 0.0, 0.0, 0.0, 0.0\n",
        "        print(f\"Case => Batch Size: {batch_size} | Input Size: {input_size} | Hidden Size: {hidden_size} | Learning Rate: {lr}\")\n",
        "        for t in range(1, TRY + 1):\n",
        "          tr_a, tr_l, te_a, te_l = train_test(batch_size, input_size, hidden_size, lr, t)\n",
        "          train_acc += tr_a\n",
        "          train_loss += tr_l\n",
        "          test_acc += te_a\n",
        "          test_loss += te_l\n",
        "        train_acc /= TRY\n",
        "        train_loss /= TRY\n",
        "        test_acc /= TRY\n",
        "        test_loss /= TRY\n",
        "        print(f\"Total Average => Train Acc: {train_acc} | Train Loss: {train_loss} | Test Acc: {test_acc} | Test Loss: {test_loss}\")\n",
        "        print(\"\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case => Batch Size: 64 | Input Size: 32 | Hidden Size: 64 | Learning Rate: 0.001\n",
            "Try: 1 | Train Acc: 82.94647979736328 | Train Loss: 0.37750716598666445\n",
            "Try: 1 | Test Acc: 72.2727279663086 | Test Loss: 0.6631170809268951\n",
            "Try: 2 | Train Acc: 82.49516296386719 | Train Loss: 0.3897367855724023\n",
            "Try: 2 | Test Acc: 72.2727279663086 | Test Loss: 0.619026243686676\n",
            "Try: 3 | Train Acc: 81.49580383300781 | Train Loss: 0.39248127900824253\n",
            "Try: 3 | Test Acc: 73.63636016845703 | Test Loss: 0.6113536283373833\n",
            "Total Average => Train Acc: 82.31248474121094 | Train Loss: 0.3865750768557698 | Test Acc: 72.72727966308594 | Test Loss: 0.6311656509836515\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 32 | Hidden Size: 64 | Learning Rate: 0.003\n",
            "Try: 1 | Train Acc: 83.5267562866211 | Train Loss: 0.35250176063605715\n",
            "Try: 1 | Test Acc: 71.36363220214844 | Test Loss: 0.664170503616333\n",
            "Try: 2 | Train Acc: 83.36557006835938 | Train Loss: 0.3522653561465594\n",
            "Try: 2 | Test Acc: 72.2727279663086 | Test Loss: 0.6832406967878342\n",
            "Try: 3 | Train Acc: 85.68665313720703 | Train Loss: 0.32847653511835606\n",
            "Try: 3 | Test Acc: 69.09090423583984 | Test Loss: 0.7507014274597168\n",
            "Total Average => Train Acc: 84.1929931640625 | Train Loss: 0.34441455063365756 | Test Acc: 70.90908813476562 | Test Loss: 0.699370875954628\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 32 | Hidden Size: 64 | Learning Rate: 0.005\n",
            "Try: 1 | Train Acc: 84.68729400634766 | Train Loss: 0.3372854155545332\n",
            "Try: 1 | Test Acc: 71.36363220214844 | Test Loss: 0.6990288347005844\n",
            "Try: 2 | Train Acc: 84.558349609375 | Train Loss: 0.3317344854680859\n",
            "Try: 2 | Test Acc: 73.18181610107422 | Test Loss: 0.7078616321086884\n",
            "Try: 3 | Train Acc: 85.17085266113281 | Train Loss: 0.3305615554658734\n",
            "Try: 3 | Test Acc: 71.81817626953125 | Test Loss: 0.5889145359396935\n",
            "Total Average => Train Acc: 84.80549621582031 | Train Loss: 0.3331938188294975 | Test Acc: 72.12120819091797 | Test Loss: 0.6652683342496554\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 32 | Hidden Size: 128 | Learning Rate: 0.001\n",
            "Try: 1 | Train Acc: 83.97807312011719 | Train Loss: 0.3542011732957801\n",
            "Try: 1 | Test Acc: 72.2727279663086 | Test Loss: 0.6282310411334038\n",
            "Try: 2 | Train Acc: 84.1392593383789 | Train Loss: 0.35401169408340843\n",
            "Try: 2 | Test Acc: 69.54545593261719 | Test Loss: 0.6161952018737793\n",
            "Try: 3 | Train Acc: 84.01031494140625 | Train Loss: 0.35005385595925\n",
            "Try: 3 | Test Acc: 72.2727279663086 | Test Loss: 0.6200366914272308\n",
            "Total Average => Train Acc: 84.04255676269531 | Train Loss: 0.3527555744461462 | Test Acc: 71.36363983154297 | Test Loss: 0.6214876448114713\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 32 | Hidden Size: 128 | Learning Rate: 0.003\n",
            "Try: 1 | Train Acc: 84.91295623779297 | Train Loss: 0.32829938372787165\n",
            "Try: 1 | Test Acc: 68.18181610107422 | Test Loss: 0.703660398721695\n",
            "Try: 2 | Train Acc: 85.2998046875 | Train Loss: 0.3290837431440548\n",
            "Try: 2 | Test Acc: 69.09090423583984 | Test Loss: 0.7211765497922897\n",
            "Try: 3 | Train Acc: 84.4938735961914 | Train Loss: 0.34276039533469144\n",
            "Try: 3 | Test Acc: 69.09090423583984 | Test Loss: 0.6996379196643829\n",
            "Total Average => Train Acc: 84.90221405029297 | Train Loss: 0.33338117406887263 | Test Acc: 68.78787231445312 | Test Loss: 0.7081582893927892\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 32 | Hidden Size: 128 | Learning Rate: 0.005\n",
            "Try: 1 | Train Acc: 85.91231536865234 | Train Loss: 0.32045224461020255\n",
            "Try: 1 | Test Acc: 70.90908813476562 | Test Loss: 0.6674070507287979\n",
            "Try: 2 | Train Acc: 85.94454956054688 | Train Loss: 0.3188166292954464\n",
            "Try: 2 | Test Acc: 67.2727279663086 | Test Loss: 0.7085254341363907\n",
            "Try: 3 | Train Acc: 85.26756286621094 | Train Loss: 0.3322773198692166\n",
            "Try: 3 | Test Acc: 69.54545593261719 | Test Loss: 0.7465905696153641\n",
            "Total Average => Train Acc: 85.7081527709961 | Train Loss: 0.3238487312582885 | Test Acc: 69.242431640625 | Test Loss: 0.7075076848268509\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 64 | Hidden Size: 64 | Learning Rate: 0.001\n",
            "Try: 1 | Train Acc: 83.10767364501953 | Train Loss: 0.3714444004759497\n",
            "Try: 1 | Test Acc: 71.36363220214844 | Test Loss: 0.6033800169825554\n",
            "Try: 2 | Train Acc: 83.26885986328125 | Train Loss: 0.37381782702037264\n",
            "Try: 2 | Test Acc: 70.45454406738281 | Test Loss: 0.6008596047759056\n",
            "Try: 3 | Train Acc: 83.78465270996094 | Train Loss: 0.36410215314553707\n",
            "Try: 3 | Test Acc: 71.81817626953125 | Test Loss: 0.6457032710313797\n",
            "Total Average => Train Acc: 83.3870620727539 | Train Loss: 0.3697881268806198 | Test Acc: 71.21212005615234 | Test Loss: 0.6166476309299469\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 64 | Hidden Size: 64 | Learning Rate: 0.003\n",
            "Try: 1 | Train Acc: 84.17150115966797 | Train Loss: 0.34086902105078404\n",
            "Try: 1 | Test Acc: 72.7272720336914 | Test Loss: 0.5888431444764137\n",
            "Try: 2 | Train Acc: 84.91295623779297 | Train Loss: 0.33420179176087284\n",
            "Try: 2 | Test Acc: 73.63636016845703 | Test Loss: 0.5877270177006721\n",
            "Try: 3 | Train Acc: 85.2998046875 | Train Loss: 0.33526777460867047\n",
            "Try: 3 | Test Acc: 72.2727279663086 | Test Loss: 0.653610959649086\n",
            "Total Average => Train Acc: 84.79475402832031 | Train Loss: 0.33677952914010917 | Test Acc: 72.8787841796875 | Test Loss: 0.6100603739420573\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 64 | Hidden Size: 64 | Learning Rate: 0.005\n",
            "Try: 1 | Train Acc: 84.52610778808594 | Train Loss: 0.3407885231533829\n",
            "Try: 1 | Test Acc: 73.63636016845703 | Test Loss: 0.6253542751073837\n",
            "Try: 2 | Train Acc: 85.17085266113281 | Train Loss: 0.3323192128113338\n",
            "Try: 2 | Test Acc: 70.45454406738281 | Test Loss: 0.6373466178774834\n",
            "Try: 3 | Train Acc: 85.10637664794922 | Train Loss: 0.3335164891821997\n",
            "Try: 3 | Test Acc: 75.0 | Test Loss: 0.6146465390920639\n",
            "Total Average => Train Acc: 84.9344482421875 | Train Loss: 0.33554140838230545 | Test Acc: 73.03030395507812 | Test Loss: 0.625782477358977\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 64 | Hidden Size: 128 | Learning Rate: 0.001\n",
            "Try: 1 | Train Acc: 84.17150115966797 | Train Loss: 0.3558015020526185\n",
            "Try: 1 | Test Acc: 73.63636016845703 | Test Loss: 0.6338092386722565\n",
            "Try: 2 | Train Acc: 84.10702514648438 | Train Loss: 0.3446112256877276\n",
            "Try: 2 | Test Acc: 74.09090423583984 | Test Loss: 0.6505459025502205\n",
            "Try: 3 | Train Acc: 83.59123229980469 | Train Loss: 0.3588140339267497\n",
            "Try: 3 | Test Acc: 71.81817626953125 | Test Loss: 0.6314831376075745\n",
            "Total Average => Train Acc: 83.95658874511719 | Train Loss: 0.35307558722236526 | Test Acc: 73.18181610107422 | Test Loss: 0.6386127596100172\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 64 | Hidden Size: 128 | Learning Rate: 0.003\n",
            "Try: 1 | Train Acc: 85.0096664428711 | Train Loss: 0.33668422455690344\n",
            "Try: 1 | Test Acc: 71.36363220214844 | Test Loss: 0.6342764496803284\n",
            "Try: 2 | Train Acc: 85.65441131591797 | Train Loss: 0.32439079181272157\n",
            "Try: 2 | Test Acc: 72.2727279663086 | Test Loss: 0.6996262967586517\n",
            "Try: 3 | Train Acc: 85.3642807006836 | Train Loss: 0.3270600517185367\n",
            "Try: 3 | Test Acc: 72.7272720336914 | Test Loss: 0.6391182839870453\n",
            "Total Average => Train Acc: 85.34278869628906 | Train Loss: 0.32937835602938725 | Test Acc: 72.12120819091797 | Test Loss: 0.6576736768086752\n",
            "\n",
            "Case => Batch Size: 64 | Input Size: 64 | Hidden Size: 128 | Learning Rate: 0.005\n",
            "Try: 1 | Train Acc: 84.36492156982422 | Train Loss: 0.34796006490989606\n",
            "Try: 1 | Test Acc: 73.18181610107422 | Test Loss: 0.657575324177742\n",
            "Try: 2 | Train Acc: 85.5899429321289 | Train Loss: 0.33032130951784094\n",
            "Try: 2 | Test Acc: 73.18181610107422 | Test Loss: 0.6731158494949341\n",
            "Try: 3 | Train Acc: 86.55706024169922 | Train Loss: 0.3103355461237382\n",
            "Try: 3 | Test Acc: 70.90908813476562 | Test Loss: 0.7029311805963516\n",
            "Total Average => Train Acc: 85.50398254394531 | Train Loss: 0.3295389735171584 | Test Acc: 72.42424011230469 | Test Loss: 0.6778741180896759\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fvM1SwP_lMkV"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}